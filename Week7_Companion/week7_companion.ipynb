{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Ensembles and Evaluation\n",
    "## Cross Validation\n",
    "This week, we'll be studying in-depth methods for evaluating classifiers. We'll start by learning about cross validation, confusion matrices, and student's t-test. To study these concepts, we'll be comparing the results of a decision tree and k-nearest neighbors on the hypothyroid dataset we have previously utilized."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:11:03.912423Z",
     "start_time": "2024-07-12T00:11:02.206548Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# download data\n",
    "data_url = 'https://raw.githubusercontent.com/cse44648/cse44648/master/datasets/hypothyroid.csv'\n",
    "\n",
    "data = pd.read_csv(data_url)\n",
    "features_to_use = ['Age', 'T4U', 'TSH']\n",
    "X = data.loc[:, features_to_use] # we will only use some features\n",
    "y = data.iloc[:, -1] # get class\n",
    "X"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Age   T4U    TSH\n",
       "0     72.0  1.48   30.0\n",
       "1     15.0  1.13  145.0\n",
       "2     24.0  1.00    0.0\n",
       "3     24.0  1.04  430.0\n",
       "4     77.0  1.28    7.3\n",
       "...    ...   ...    ...\n",
       "3158  58.0  0.91    5.8\n",
       "3159  29.0  1.01    0.8\n",
       "3160  77.0  0.68    1.2\n",
       "3161  74.0  0.48    1.3\n",
       "3162  56.0  0.97    0.0\n",
       "\n",
       "[3163 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>T4U</th>\n",
       "      <th>TSH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>77.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3161</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3163 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:12:08.371204Z",
     "start_time": "2024-07-12T00:12:08.364331Z"
    }
   },
   "source": [
    "# start by preprocessing the data\n",
    "# we'll try dropping NA values first\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index] # keep the corresponding classes\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "print(y.value_counts(normalize=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291\n",
      "2291\n",
      "Class\n",
      "negative       0.941074\n",
      "hypothyroid    0.058926\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:12:24.951362Z",
     "start_time": "2024-07-12T00:12:22.893535Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), columns=features_to_use)\n",
    "X.describe()\n",
    "# note that the mean is very close to 0, but not quite - this is because Python takes shortcuts when rounding data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Age           T4U           TSH\n",
       "count  2.291000e+03  2.291000e+03  2.291000e+03\n",
       "mean   1.163045e-17 -5.458556e-16 -1.240581e-17\n",
       "std    1.000218e+00  1.000218e+00  1.000218e+00\n",
       "min   -2.717730e+00 -4.393170e+00 -2.498511e-01\n",
       "25%   -8.333142e-01 -5.783611e-01 -2.498511e-01\n",
       "50%    1.612387e-01 -1.347786e-01 -2.207920e-01\n",
       "75%    8.417223e-01  3.531621e-01 -1.585223e-01\n",
       "max    2.359724e+00  4.611554e+00  2.175208e+01"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>T4U</th>\n",
       "      <th>TSH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.291000e+03</td>\n",
       "      <td>2.291000e+03</td>\n",
       "      <td>2.291000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.163045e-17</td>\n",
       "      <td>-5.458556e-16</td>\n",
       "      <td>-1.240581e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000218e+00</td>\n",
       "      <td>1.000218e+00</td>\n",
       "      <td>1.000218e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.717730e+00</td>\n",
       "      <td>-4.393170e+00</td>\n",
       "      <td>-2.498511e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.333142e-01</td>\n",
       "      <td>-5.783611e-01</td>\n",
       "      <td>-2.498511e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.612387e-01</td>\n",
       "      <td>-1.347786e-01</td>\n",
       "      <td>-2.207920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.417223e-01</td>\n",
       "      <td>3.531621e-01</td>\n",
       "      <td>-1.585223e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.359724e+00</td>\n",
       "      <td>4.611554e+00</td>\n",
       "      <td>2.175208e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split our data into training and testing folds using 5-fold validation. In this case, the data set is split into 5 equal-sized partitions. In each fold, 1/5 of the data is used as testing data, and the other 4/5 of the data is used as training data. Each instance will thus be part of the testing set exactly once, and the training set four times."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:14:04.532546Z",
     "start_time": "2024-07-12T00:14:04.245545Z"
    }
   },
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=False) # initialize the KFold object\n",
    "splits = kfold.split(X) # call the split method on our feature set\n",
    "for train_idx, test_idx in list(splits):\n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    print('Train set size: {}; test set size: {}'.format(len(X_train), len(X_test)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1832; test set size: 459\n",
      "Train set size: 1833; test set size: 458\n",
      "Train set size: 1833; test set size: 458\n",
      "Train set size: 1833; test set size: 458\n",
      "Train set size: 1833; test set size: 458\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a KNN and decision tree model for each fold, and compare their average performance across each of the folds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:14:51.404254Z",
     "start_time": "2024-07-12T00:14:50.861774Z"
    }
   },
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True) # initialize the KFold object\n",
    "splits = kfold.split(X) # call the split method on our feature set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "knn_results = []\n",
    "dt_results = []\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    train_idx, test_idx = split\n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    \n",
    "    # each time we call the .fit() function on a model, it overwrites any previous training.\n",
    "    knn.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # generate predictions and calculate accuracy\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_acc = np.sum(knn_pred == y_test) / len(y_test)\n",
    "    knn_results.append(knn_acc)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_acc = np.sum(dt_pred == y_test) / len(y_test)\n",
    "    dt_results.append(dt_acc)\n",
    "    \n",
    "    print('Test fold {} results:'.format(i + 1))\n",
    "    print('\\tKNN: {:.4f}'.format(knn_acc))\n",
    "    print('\\t DT: {:.4f}'.format(dt_acc))\n",
    "    \n",
    "print('Average results:')\n",
    "print('\\tKNN: {:.4f} +- {:.4f}'.format(np.mean(knn_results), np.std(knn_results)))\n",
    "print('\\t DT: {:.4f} +- {:.4f}'.format(np.mean(dt_results), np.std(dt_results)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 results:\n",
      "\tKNN: 0.9586\n",
      "\t DT: 0.9521\n",
      "Test fold 2 results:\n",
      "\tKNN: 0.9563\n",
      "\t DT: 0.9607\n",
      "Test fold 3 results:\n",
      "\tKNN: 0.9672\n",
      "\t DT: 0.9585\n",
      "Test fold 4 results:\n",
      "\tKNN: 0.9607\n",
      "\t DT: 0.9498\n",
      "Test fold 5 results:\n",
      "\tKNN: 0.9607\n",
      "\t DT: 0.9541\n",
      "Average results:\n",
      "\tKNN: 0.9607 +- 0.0036\n",
      "\t DT: 0.9550 +- 0.0040\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average and standard deviation of a classifier across multiple testing sets provides a more robust measure of performance, since the classifier is tested on a variety of samples. Also, the low standard deviation means that there is low variance between the samples (folds). However, rather than just eyeballing these numbers, let's use statistical methods to decide whether the differences are significant.\n",
    "\n",
    "## Student's T-test\n",
    "Student's t-test is a statistical hypothesis test that is used to determine whether there is a statistically significant difference between the values of 2 samples. In this case, our 2 samples are the cross validation scores for the KNN and decision tree models. The null hypothesis is that there is no significant difference between the outputs of the two classifiers. Based on the result of the t-test we can either reject the null hypothesis (if the p-value is low enough) or fail to reject it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:23:44.922292Z",
     "start_time": "2024-07-12T00:23:44.917032Z"
    }
   },
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sample1 = knn_results\n",
    "sample2 = dt_results\n",
    "\n",
    "print(sample1)\n",
    "print(sample2)\n",
    "\n",
    "ttest_ind(sample1, sample2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9694989106753813, 0.9650655021834061, 0.9344978165938864, 0.9541484716157205, 0.9650655021834061]\n",
      "[0.9520697167755992, 0.9606986899563319, 0.9585152838427947, 0.9497816593886463, 0.9541484716157205]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.39383979188172374, pvalue=0.703988507847489)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the p-value is .49 - well above the typical threshold of 0.05 for significance. Therefore, we fail to reject the null hypothesis - i.e., we say that there is no statistically significant difference between performance of the two classifiers.\n",
    "\n",
    "Let's try comparing KNN to Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:06:27.269261Z",
     "start_time": "2024-07-18T14:06:26.728545Z"
    }
   },
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True) # initialize the KFold object\n",
    "splits = kfold.split(X) # call the split method on our feature set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "nb = GaussianNB()\n",
    "\n",
    "knn_results = []\n",
    "nb_results = []\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    train_idx, test_idx = split\n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    \n",
    "    # each time we call the .fit() function on a model, it overwrites any previous training.\n",
    "    knn.fit(X_train, y_train)\n",
    "    nb.fit(X_train, y_train)\n",
    "    \n",
    "    # generate predictions and calculate accuracy\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_acc = np.sum(knn_pred == y_test) / len(y_test)\n",
    "    knn_results.append(knn_acc)\n",
    "    nb_pred = dt.predict(X_test)\n",
    "    nb_acc = np.sum(nb_pred == y_test) / len(y_test)\n",
    "    nb_results.append(nb_acc)\n",
    "    \n",
    "    print('Test fold {} results:'.format(i + 1))\n",
    "    print('\\tKNN: {:.4f}'.format(knn_acc))\n",
    "    print('\\t NB: {:.4f}'.format(nb_acc))\n",
    "    \n",
    "print('Average results:')\n",
    "print('\\tKNN: {:.4f} +- {:.4f}'.format(np.mean(knn_results), np.std(knn_results)))\n",
    "print('\\t NB: {:.4f} +- {:.4f}'.format(np.mean(nb_results), np.std(nb_results)))\n",
    "print()\n",
    "print(ttest_ind(knn_results, nb_results))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaive_bayes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GaussianNB\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ttest_ind\n\u001B[0;32m----> 4\u001B[0m kfold \u001B[38;5;241m=\u001B[39m \u001B[43mKFold\u001B[49m(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# initialize the KFold object\u001B[39;00m\n\u001B[1;32m      5\u001B[0m splits \u001B[38;5;241m=\u001B[39m kfold\u001B[38;5;241m.\u001B[39msplit(X) \u001B[38;5;66;03m# call the split method on our feature set\u001B[39;00m\n\u001B[1;32m      7\u001B[0m knn \u001B[38;5;241m=\u001B[39m KNeighborsClassifier(n_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the p-value is well below the 0.05 significance threshold. We can therefore reject the null hypothesis that the results of the classifiers are not statistically significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "While the t-test is useful for comparing the performance of classifiers, performance as measured by accuracy does not tell us about the type of errors our classifier makes. Above we saw that about 94% of the data has the class \"negative\" which means that any classifier that simply predicted \"hypothyroid\" would achieve an accuracy of around 94%. To better understand the types of errors the classifier makes, we can use a confusion matrix. In this example, we will show the confusion matrix from the last testing fold in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:28:27.899404Z",
     "start_time": "2024-07-12T00:28:27.892216Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion matrix for KNN')\n",
    "print(confusion_matrix(y_test, knn_pred))\n",
    "print('\\nConfusion matrix for DT')\n",
    "print(confusion_matrix(y_test, dt_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for KNN\n",
      "[[ 11  14]\n",
      " [  8 425]]\n",
      "\n",
      "Confusion matrix for DT\n",
      "[[ 16   9]\n",
      " [ 14 419]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T00:33:56.715143Z",
     "start_time": "2024-07-12T00:33:56.682249Z"
    }
   },
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# sklearn also includes plot_confusion_matrix, which does the predictions and computes the matrix for us\n",
    "ConfusionMatrixDisplay(knn, X_test, y_test)\n",
    "ConfusionMatrixDisplay(dt, X_test, y_test)"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfusionMatrixDisplay\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# sklearn also includes plot_confusion_matrix, which does the predictions and computes the matrix for us\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mConfusionMatrixDisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mknn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m ConfusionMatrixDisplay(dt, X_test, y_test)\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a summary of the types of errors made by a classifier. The y axis represents the true label, and the x axis represents the predicted label. The top left cell represents the number of testing instances that had a true label of \"hypothyroid\" for which the classifier correctly predicted \"hypothyroid\" - the true positives. The bottom left cell shows false positives, the top right cell shows false negatives, and the bottom right cell shows true negatives.\n",
    "\n",
    "In the lecture we will discuss precision, recall, ROC curves, and other tools that measure a classifier based on the types of errors shown in this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
